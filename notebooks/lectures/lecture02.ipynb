{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Physical Oceanographic Data - SIO 221A\n",
    "### Python version of [Sarah Gille's](http://pordlabs.ucsd.edu/sgille/sioc221a/index.html) notes by:\n",
    "#### Bia Villas Bôas (avillasboas@ucsd.edu) & Gui Castelão (castelao@ucsd.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2:  Probing the power of probability density functions\n",
    "\n",
    "### **Recap**\n",
    "\n",
    "Last time we talked about some basic statistical measures.  If we\n",
    "have a collection of data (*random variables*), we can compute\n",
    "their *mean*, *variance*, *standard deviation*, *median*,\n",
    "and we can examine the *probability density function* of\n",
    "the data.  We also made a distinction between the *expectation value*\n",
    "(the value we'd expect if we had an infinite number of perfectly sampled\n",
    "observations) and the observed mean.  Similarly, we can distinguish\n",
    "between an empirical probability density function (what we actually\n",
    "observe) and the idea probability density function that we observe.\n",
    "\n",
    "### **An example**\n",
    "\n",
    "Melissa Carter, the Shore Stations Program manager, is in charge of the\n",
    "pier data.  Next week she'll tell us in detail about the data collection\n",
    "system.  The system includes automated sensors that suffer from\n",
    "biofouling and manual time series that are under sampled.  Melissa\n",
    "asks, \"What new information is gained with the continuous 4min time series, and is there a need to continue to collect the manual once per day measurements?\"\n",
    "\n",
    "We could start to answer these questions by using the tools we reviewed\n",
    "in the first lecture.  Are the means the same?  Are the variances the same?\n",
    "But that will give us an incomplete picture for several reasons:\n",
    "\n",
    "1. As we noted last time, data sets can perversely have the same mean and\n",
    "standard deviation, but have pdfs that look nothing alike.\n",
    "2. When we deal with real data, nothing is ever identical, so we'll need\n",
    "to know how big a difference is acceptable.\n",
    "\n",
    "The pdf is going to help us work through these issues.\n",
    "What can we do with a pdf?  Let's cover three topics:\n",
    "\n",
    "1. How do we define a pdf?\n",
    "2. How do we use the pdf to think about confidence limits?  Are two estimates different?\n",
    "3. How can we tell if two pdfs are different?\n",
    "\n",
    "The formal definition of a probability density function is based on\n",
    "the first derivative of the probability:\n",
    "\n",
    "$$\\begin{equation}\n",
    "p(x) = \\lim_{\\Delta x \\to 0}\\left[\\frac{\\textrm{Prob}[x < x(k) \\le x+\\Delta x]}{\\Delta x}\\right] \\hspace{3cm} (1)\n",
    "\\end{equation}$$\n",
    "\n",
    "where $x(k)$ is a random variable.  This definition means that the integral of\n",
    "the probability density function gives us the probability, as we noted last\n",
    "time:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\int_{-\\infty}^{\\infty} p(x)\\, dx = 1\\hspace{3cm} (2)\n",
    "\\end{equation}$$\n",
    "\n",
    "One of the clever aspects of the pdf is that we can use it to determine\n",
    "an expected value:\n",
    "\n",
    "$$\\begin{equation}\n",
    "E(x(k)) = \\int_{-\\infty}^\\infty xp(x)\\, dx = \\mu_x.\\hspace{3cm} (3)\n",
    "\\end{equation}$$\n",
    "\n",
    "Why does this work?  In essence, I reorder all the values in my data set and\n",
    "ask what's the probability of finding $x$ in bin 1, what's the probability\n",
    "of finding $x$ in bin 2, etc?  Or in other words, what fraction of my total\n",
    "record is in bin 1, what fraction is in bin 2, etc?  And summing this way,\n",
    "I'll find the mean.\n",
    "\n",
    "We can also use this for $x^2$ or for $(x-\\mu_x)^2$.\n",
    "\n",
    "$$\\begin{equation}\n",
    "E((x(k)-\\mu_k)^2) = \\int_{-\\infty}^\\infty (x-\\mu_x)^2 p(x)\\, dx = \\sigma_x^2. \\hspace{3cm} (4)\n",
    "\\end{equation}$$\n",
    "\n",
    "\n",
    "Let's start by making an empirical pdf.  Last time we talked about\n",
    "temperature on the pier, so now, let's take a look at pressure.  We could\n",
    "plot a histogram of the data using the hist function, but that wouldn't\n",
    "give us a pdf.  For the pdf we need to be properly normalized.  We can\n",
    "still do this using functions from the `matplotlib` and `numpy` libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sccoos_thredds = 'http://sccoos.org/thredds/dodsC/autoss/scripps_pier-2019.nc'\n",
    "ds = xr.open_dataset(sccoos_thredds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check what variables this dataset has:\n",
    "ds.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we're interested in pressure, so let's check the characteristics of this variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure = ds['pressure'].values\n",
    "print(ds['pressure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could plot a histogram using the `matplotlib` function `hist` which allows us to specify the argument `density`. Let's see how to get both histogram and pdf of the pressure using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize = (16, 8))\n",
    "hist = ax1.hist(ds['pressure'].values, bins=30, density=False)\n",
    "ax1.set_ylabel('Counts', fontsize=14)\n",
    "ax1.set_xlabel('Pressure [%s]'%ds['pressure'].units, fontsize=14) # using the metadata\n",
    "ax1.set_title('Histogram', fontsize=18)\n",
    "# Now, with density=True\n",
    "pdf  = ax2.hist(ds['pressure'].values, bins=30, density=True)\n",
    "ax2.set_ylabel('Probability density', fontsize=14)\n",
    "ax2.set_xlabel('Pressure [%s]'%ds['pressure'].units, fontsize=14)\n",
    "ax2.set_title('PDF', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check if `density=True` is working properly. The function `hist` returns a tuple, where the first two elements are counts/pdf and bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, bins, _ = pdf\n",
    "dbin = bins[1] - bins[0]\n",
    "print(values.sum()*dbin, 'is close enough to 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to have control over the bin width, so instead of passing the number of bins, you can pass a list representing the bin edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbin1 = 0.1\n",
    "bins1 = np.arange(ds['pressure'].min(), ds['pressure'].max(), dbin1).tolist()\n",
    "dbin2 = 0.01\n",
    "bins2 = np.arange(ds['pressure'].min(), ds['pressure'].max(), dbin2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize = (16, 8))\n",
    "pdf1 = ax1.hist(ds['pressure'], bins=bins1, density=True)\n",
    "ax1.set_ylabel('Probability density', fontsize=14)\n",
    "ax1.set_xlabel('Pressure [%s]'%ds['pressure'].units, fontsize=14) # using the metadata\n",
    "ax1.set_title('PDF', fontsize=18)\n",
    "# Now, with narrower bins\n",
    "pdf2  = ax2.hist(ds['pressure'], bins=bins2, density=True)\n",
    "ax2.set_ylabel('Probability density', fontsize=14)\n",
    "ax2.set_xlabel('Pressure [%s]'%ds['pressure'].units, fontsize=14)\n",
    "ax2.set_title('PDF', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My predecessor, Rob Pinkel, always told students that they couldn't use the ``hist`` function for this and should do a loop.  How do we do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbin = 0.1\n",
    "bin_min = 2\n",
    "bin_max = 5\n",
    "bins = np.arange(bin_min, bin_max, dbin)\n",
    "count = []\n",
    "for i in range(len(bins)):\n",
    "    ind = (pressure>bins[i] - dbin/2) & (pressure<=bins[i]+dbin/2)\n",
    "    count.append(ind.sum())\n",
    "count = np.array(count)\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(bins, count)\n",
    "plt.xlabel('Pressure [dbar]', fontsize=14)\n",
    "plt.ylabel('counts',  fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we write a function to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_histogram(variable, bin_max, bin_min, dbin, pdf=False):\n",
    "    \n",
    "    \"\"\" Computes 1D histogram or probability density for a given variable.\n",
    "        \n",
    "    Keyword arguments:\n",
    "    variable -- 1D array.\n",
    "    bin_max -- maximum value for bins\n",
    "    bin_min -- minimum value for bins\n",
    "    dbin -- bin size\n",
    "    pdf -- (default False)\n",
    "    \n",
    "    Returns:\n",
    "    bins -- histogram bins\n",
    "    counts -- either counts or probability density\n",
    "        \n",
    "    \"\"\"\n",
    "    bins = np.arange(bin_min, bin_max, dbin)\n",
    "    count = []\n",
    "    for i in range(len(bins)):\n",
    "        ind = (variable>bins[i] - dbin/2) & (variable<=bins[i]+dbin/2)\n",
    "        count.append(ind.sum())\n",
    "    count = np.array(count)\n",
    "    if pdf:\n",
    "        norm_hist = count/count.sum()/dbin\n",
    "        assert np.allclose(norm_hist.sum()*dbin, 1.0), \"PDF doesn't sum to 1\"\n",
    "    \n",
    "        return bins, norm_hist\n",
    "    else:\n",
    "        return bins, count    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We like to think that geophysical variables are normally distributed,\n",
    "meaning that the distribution is:\n",
    "\n",
    "$$\\begin{equation}\n",
    "p(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\hspace{3cm} (5)\n",
    "\\end{equation}$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "So we can add a Gaussian to our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins, norm_hist = compute_histogram(variable=pressure, bin_max=pressure.max(),\n",
    "                                    bin_min=pressure.min(), dbin=0.1, pdf=True)\n",
    "\n",
    "sig = np.std(pressure)\n",
    "mu = np.mean(pressure)\n",
    "gaus = (1.0/sig/np.sqrt(2*np.pi)) * (np.exp(-(bins-mu)**2 / (2*sig**2)))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(bins, norm_hist, 'b', lw=2, label='Data')\n",
    "plt.plot(bins, gaus, 'k', lw=2, label='Gaussian')\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel('Pressure [dbar]', fontsize=14)\n",
    "plt.ylabel('Probability density',  fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We like the Gaussian, because it's easy to calculate, and it has well\n",
    "defined properties.  We know that 68\\% of measurements will be within\n",
    "$\\pm \\sigma$ of the mean, and 95\\% of measurements will be within $\\pm 2\\sigma$\n",
    "of the mean.\n",
    "\n",
    "We can turn this around to decide whether a measurement is\n",
    "an outlier.  If we expect to see a lot of values near the mean, and we find that\n",
    "we have a measurement that deviates from the mean by 5 $\\sigma$, then it's\n",
    "not terribly statistically likely.  (For a Gaussian, 99.99994\\% of observations\n",
    "should be within $\\pm 5\\sigma$ of the mean.)  Thus we might decide to throw\n",
    "out all outliers that differ from the mean by more than 3 or 4 or 5$\\sigma$.\n",
    "\n",
    "We can also use this framework to think about uncertainty.  If we measure\n",
    "one realization of an estimate of the mean, that will become our best estimate\n",
    "of the mean.  If our formal estimate of our a priori uncertainty is correct\n",
    "(and we might also call this $\\sigma$, but let's use $\\delta$ for now),\n",
    "then we expect that\n",
    "68\\% of the time, our single observation should be within $\\pm\\delta$ of the\n",
    "true value, and 95\\% of the time, our single observation should be wihtin\n",
    "$\\pm2\\delta$ of the true value.\n",
    "\n",
    "And really, we like the Gaussian, because the convolution of a Gaussian\n",
    "with another Gaussian is still a Gaussian, so we can manipulate the\n",
    "statistics easily.  But are data necessarily normally distributed? So this might lead you to think that all data are fairly Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example pdfs of real data? Non-Gaussian cases.\n",
    "Now what if we plot chlorophyll?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality control of the Chlorophyll measurements in this dataset is provided in a separate variable `chlorophyll_flagPrimary`. So let's check what is the meaning of this flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['chlorophyll_flagPrimary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like the flag for good data is 1. Below we sellect only thw good values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = ds['chlorophyll_flagPrimary'].values==1\n",
    "good_chl = ds['chlorophyll'].values[flag]\n",
    "bins = np.linspace(good_chl.min(), good_chl.max(), 30)\n",
    "chl_mu = np.mean(good_chl)\n",
    "chl_sig = np.std(good_chl)\n",
    "chl_gaus = (1.0/chl_sig/np.sqrt(2*np.pi)) * (np.exp(-(bins-chl_mu)**2 / (2*chl_sig**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "hist = plt.hist(good_chl, bins=bins, density=True, label = 'Observations')\n",
    "plt.plot(bins, chl_gaus, 'k', lw=2, label='Gaussian')\n",
    "plt.xlabel('Chlorophyll concentration  [$\\mu$ g/L]', fontsize=14)\n",
    "plt.ylabel('Probability density',  fontsize=14)\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated above, chlorophyll concentrations are decidedly non-Gaussian.  (We usually\n",
    "refer to chlorophyll as being log-normally distributed, meaning that the\n",
    "log of the values might be Gaussian.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ocean velocity data often have a double-exponential distribution, as do\n",
    "wind velocity data:\n",
    "\n",
    "$$\\begin{equation}\n",
    "p(x)=\\frac{1}{\\sigma\\sqrt{2}} \\exp{\\left[-\\frac{\\left|x\\right|\\sqrt{2}}{\\sigma} \\right]}. \\hspace{3cm} (6)\n",
    "\\end{equation}$$\n",
    "\n",
    "Sometimes we only measure wind speed, and that's necessarily positive.\n",
    "The Rayleigh distribution is sometimes a good representation of wind\n",
    "speed:  it is defined from the square root sum of two independent\n",
    "Gaussian components squared, $y=\\sqrt{x_1^2 + x_2^2}$.\n",
    "$$\\begin{equation}\n",
    "p(y)=\\frac{y}{\\sigma^2} \\exp{\\left[-\\frac{y^2}{2\\sigma^2} \\right]}.\\hspace{3cm} (7)\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing variables, error propagation, and the central limit theorem\n",
    "\n",
    "Given that so many pdfs can be non-Gaussian, why do we spend so much\n",
    "time talking about Gaussians?  There are two important reasons.\n",
    "\n",
    "1. As noted above, the Gaussian is mathematically tractable.\n",
    "2. Even though individual pdfs are non-Gaussian, if we sum enough variables, everything is Gaussian.  (This is the central limit theorem, which we'll get to next time.)\n",
    "\n",
    "\n",
    "Often the quantities we study represent a summation of multiple\n",
    "random variables.  For example, we're not interested in the instantaneous\n",
    "temperature but the average over an hour or a day.  Thus we consider\n",
    "\n",
    "$$\\begin{equation}\n",
    "x(k) = \\sum_{i=1}^{N} a_i x_i(k),\\hspace{3cm} (8)\n",
    "\\end{equation}$$\n",
    "\n",
    "following the terminology of Bendat and Piersol, where $a_i$ is a\n",
    "coefficient. The mean of $x$ is\n",
    "$$\\begin{equation}\n",
    "\\mu_x = E(x(k)) = E\\left[\\sum_{i=1}^{N} a_i x_i(k)\\right] = \\left[\\sum_{i=1}^{N} a_i E(x_i(k))\\right] =\n",
    "\\sum_{i=1}^{N} a_i \\mu_i. \\hspace{3cm} (9)\n",
    "\\end{equation}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\sigma_x^2 = E\\left[(x(k)-\\mu_x)^2\\right] =\n",
    "=E\\left[\\sum_{i=1}^{N} a_i (x_i(k)-\\mu_i)\\right]^2\n",
    "= \\sum_{i=1}^{N} a_i^2 \\sigma_i^2. \\hspace{3cm} (10)\n",
    "\\end{equation}$$\n",
    "\n",
    "In doing this, we've carried out a little sleight of hand, by assuming\n",
    "that for a large ensemble (as the number of elements used to define our\n",
    "expectation value $E$ approaches $\\infty$) the correlation between\n",
    "$x_i$ and $x_j$ is zero so that the expectation value\n",
    "$E[(x_i(k)-\\mu_i)(x_j(k)-\\mu_j)] = 0$ for $i\\ne j$.\n",
    "\n",
    "This gives us some simple rules of thumb:\n",
    "\n",
    "#### *Standard error of the mean.*\n",
    "Suppose that $a_i$ is an averaging operator and is equal to $1/N$, and\n",
    "$\\sigma_i$ is the same for all $i$.  Then\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\sigma_x^2 =  \\sum_{i=1}^{N} a_i^2 \\sigma_i^2 = \\frac{N\\sigma_i^2}{N^2} =\n",
    "\\frac{\\sigma_i^2}{N}.\\hspace{3cm} (11)\n",
    "\\end{equation}$$\n",
    "\n",
    "This means that the standard deviation of the mean, {\\it the standard error\n",
    "of the mean}, is $\\sigma/\\sqrt{N}$.\n",
    "\n",
    "As a footnote to this, the {\\it standard error of the variance} is\n",
    "$\\sigma^2\\sqrt{2/(N-1)}$.\n",
    "\n",
    "#### *Error Propagation*\n",
    "Our consideration of the summed variables\n",
    "gives us a rule for estimating uncertainties of computed quantities.  If\n",
    "we sum a variety of measures together, then the overall uncertainty\n",
    "will be determined by the square root of the sum of the squares:\n",
    "\\begin{equation}\n",
    "\\delta_y = \\sqrt{\\sum_{i=1}^{N} a_i^2 \\delta_i^2},\n",
    "\\end{equation}\n",
    "where here we're using $\\delta_i$ to represent the a priori uncertainties. \n",
    "\n",
    "What if we have to multiply quantities together?  Then we simply\n",
    "linearize about the value of interest. We'll do this properly next time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
