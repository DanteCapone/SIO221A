{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Physical Oceanographic Data - SIO 221A\n",
    "### Python version of [Sarah Gille's](http://pordlabs.ucsd.edu/sgille/sioc221a/index.html) notes by:\n",
    "#### Bia Villas Bôas (avillasboas@ucsd.edu) & Gui Castelão (castelao@ucsd.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4:  \n",
    "\n",
    "### **Recap**\n",
    "Key concepts from last time focused on the standard error of the mean\n",
    "and its links to error propagation, the chi-squared and Rayleigh distributions,\n",
    "and methods for telling whether one pdf is statistically like another\n",
    "(the Komogorov-Smirnov test and the chi-squared test).\n",
    "\n",
    "You'll recall that the Rayleigh distribution represents the square\n",
    "root of two independent Gaussian components, $y=\\sqrt{x_1^2 + x_2^2}$.\n",
    "\n",
    "$$\\begin{equation}\n",
    "p(y)=\\frac{y}{\\sigma^2} \\exp{\\left[-\\frac{y^2}{2\\sigma^2} \\right]}. \\hspace{3cm} (1)\n",
    "\\end{equation}$$\n",
    "\n",
    "And the $\\chi^2$ distribution represents the sum of $n$ squared\n",
    "variables:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\chi_n^2 = z_1^2 + z_2^2 + z_3^2 + ... + z_n^2.\\hspace{3cm} (1)\n",
    "\\end{equation}$$\n",
    "\n",
    "The slides showed examples of these, and we noted that the $\\chi^2$ distribution\n",
    "has a mathematical formulation in terms of the $\\Gamma$ function.  I glossed\n",
    "over the definition of the $\\Gamma$ function, because it's not very\n",
    "mathematically tractable, and is normally handled via a look-up table.\n",
    "\n",
    "We also looked at two possible tests for telling if two pdfs differ.  One\n",
    "was the Kolmogorov-Smirnov test.  (If you have one data set, and want to\n",
    "know whether it is plausibly Gaussian, put all the data\n",
    "in a vector $\\bf{x}$, and in Python use the function ``kstest(x)`` from ``scipy.stats``.\n",
    "In Python, the function ``kstest`` outputs the actual statistics as oposed to a binary answer (which is the case in Matlab). To decide wheter of not to reject the null hypothesis you have to look at both the KS statistics and the p-value. So for example, if we want to test if an array of Gaussian noise has a Gaussian\n",
    "distribution, we could type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frist let's try with 1000 points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "noise = np.random.randn(N)\n",
    "plt.hist(noise, 30)\n",
    "print(stats.kstest(noise, 'norm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KS score is somewhat close to zero (but not quite zero) and the p-value is large, meaning that we cannot reject the hypothesis that the samples were drawn from the same distribution). Now let's try with a larger sample (N= 100,000 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "noise = np.random.randn(N)\n",
    "plt.hist(noise, 30)\n",
    "print(stats.kstest(noise, 'norm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a non-Gaussian (uniform distribution) of random numbers we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "noise = np.random.rand(N)\n",
    "plt.hist(noise, 30)\n",
    "print(stats.kstest(noise, 'norm', N=100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the KS statistics is high and the p-value is zero (meaning that we can reject the null hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're comparing two data sets (e.g. $x$ and $y$),\n",
    "we can use the function``ks_2samp``.  Again, Python outputs the actual statistics and the null hipothesys is that both datasets were drawn from the same distribution. Let's compare these two cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same distribution (normal)\n",
    "N = 1000000\n",
    "x = np.random.randn(N)\n",
    "y = np.random.randn(N)\n",
    "stats.ks_2samp(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different distributions (normal and uniform)\n",
    "N = 1000000\n",
    "x = np.random.randn(N)\n",
    "y = np.random.rand(N)\n",
    "stats.ks_2samp(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second was  a $\\chi^2$ test, in which you bin your data into histograms\n",
    "and ask if the number of data you find in each bin is consistent.\n",
    "\n",
    "A second strategy is to bin the data and ask whether the number of\n",
    "data in the bin is consistent with what we'd expect, using a $\\chi^2$\n",
    "statistics.  In this case for comparisons with a theoretical pdf,\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\chi^2 = \\sum_i\\frac{(N_i-n_i)^2}{n_i},\\hspace{3cm} (3)\n",
    "\\end{equation}$$\n",
    "\n",
    "where $N_i$ is the observed number of events in bin $i$, and $n_i$ is the theoretical\n",
    "or expected number of events in bin $i$.\n",
    "For comparisons between two distributions,\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\chi^2 = \\sum_i\\frac{(N_i-M_i)^2}{N_i+M_i},\\hspace{3cm} (4)\n",
    "\\end{equation}$$\n",
    "\n",
    "where $N_i$ and $M_i$ are each observed numbers of events for bin $i$.\n",
    "The values of $\\chi^2$ are evaluated using the $\\chi^2$ probability function $Q(\\chi^2|\\nu)$, which is an incomplete gamma function,\n",
    "where $\\nu$ is the number of bins (or the number of bins minus one, depending on\n",
    "normalization).  In Python this is:\n",
    "\n",
    "``scipy.special.gammainc(nu/2, chi_squared/2)``\n",
    "\n",
    "or equivalently\n",
    "\n",
    "``scipy.stats.chi2.cdf(chi_squared, nu)``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special as spe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(-6, 6, 0.5)\n",
    "a1, a2 = np.histogram(np.random.randn(N),bins);\n",
    "b1, b2 = np.histogram(np.random.randn(N),bins);\n",
    "c1, c2 = np.histogram(np.random.randn(N),bins);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2 = np.ma.sum(np.ma.masked_invalid((b1-a1)**2/(b1+a1)))\n",
    "print('From chi2: ', stats.chi2.cdf(chi2, 25))\n",
    "print('From gammainc: ', spe.gammainc(25/2, chi2/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2 = np.ma.sum(np.ma.masked_invalid((c1-a1)**2/(c1+a1)))\n",
    "print('From chi2: ', stats.chi2.cdf(chi2, 25))\n",
    "print('From gammainc: ', spe.gammainc(25/2, chi2/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data sets are similar at least at the 95\\% level,\n",
    "then we expect ``gammainc`` to return a value less than or equal to 0.95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a function to data:  least-squares fitting\n",
    "\n",
    "Now, let's return to our time series.  You might remember we were\n",
    "looking for a linear trend for:\n",
    "\n",
    "$$\\begin{equation}\n",
    "{\\bf T}=T_o + b {\\bf t} + {\\bf n},\\hspace{3cm} (5)\n",
    "\\end{equation}$$\n",
    "\n",
    "where ${\\bf T}$ represents our measured temperature data (as a vector),\n",
    "$T_o$ is a constant (unknown), ${\\bf t}$ is time, and $b$ is the time rate of\n",
    "change (also unknown), and since this is the real world, ${\\bf n}$ is noise\n",
    "(representing the part of the signal that isn't a linear trend.\n",
    "Formally, provided that we have more than two measurements, aside from\n",
    "the unknown noise vector, this\n",
    "is an over-determined system.\n",
    "Since the noise is unknown, and there are lots of independent values,\n",
    "the system is formally\n",
    "underdetermined.  But we won't lose hope.  We just move forward under\n",
    "the assumption that the noise is small.\n",
    "\n",
    "Last time we started writing this as a matrix equation:\n",
    "\n",
    "$$\\begin{equation}\n",
    "{\\bf Ax} + {\\bf n}= {\\bf y},\\hspace{3cm} (6)\n",
    "\\end{equation}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\begin{equation}\n",
    "{\\bf A} =  \\left[\\begin{array}{cc}\n",
    "                              1 & t_1  \\\\\n",
    "                              1 & t_2  \\\\\n",
    "                              1 & t_3  \\\\\n",
    "                              \\vdots & \\vdots \\\\\n",
    "                              1 & t_N  \\end{array}\\right],\\hspace{3cm} (7)\n",
    "\\end{equation}$$\n",
    "\n",
    "making ${\\bf A}$ an $N\\times 2$ matrix.\n",
    "And ${\\bf y}$ is an $N$-element column vector containing, for example, our temperature data:\n",
    "$$\\begin{equation}\n",
    "{\\bf y} =  \\left[\\begin{array}{c}\n",
    "                              T_1  \\\\\n",
    "                              T_2  \\\\\n",
    "                              T_3  \\\\\n",
    "                              \\vdots \\\\\n",
    "                              T_N  \\end{array}\\right].\\hspace{3cm} (8)\n",
    "\\end{equation}$$\n",
    "\n",
    "Then ${\\bf x}$ is the vector of unknown coefficients, in this case with\n",
    "2 elements (e.g. $x_1=T_o$ and\n",
    "$x_2=b$).\n",
    "\n",
    "$$\\begin{equation}\n",
    "{\\bf x} =  \\left[\\begin{array}{c}\n",
    "                              x_1  \\\\\n",
    "                              x_2 \\end{array}\\right] \\hspace{3cm} (9)\n",
    "\\end{equation}$$\n",
    "\n",
    "How can we find the best solution to this equation to minimize the misfit\n",
    "between the data ${\\bf y}$ and the model ${\\bf Ax}$?   The\n",
    "misfit could be positive or negative, and absolute values aren't mathematically\n",
    "tractable, so let's start by squaring the misfit.\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\epsilon = ({\\bf Ax} - {\\bf y})^T({\\bf Ax} - {\\bf y}) = {\\bf x}^T{\\bf A}^T{\\bf Ax} - 2{\\bf x}^T{\\bf A}^T{\\bf y} + {\\bf y}^T{\\bf y}.\\hspace{3cm} (10)\n",
    "\\end{equation}$$\n",
    "\n",
    "Then we can minimize the squared misfit.  The natural route to minimization\n",
    "comes by taking the derivative, and then setting the results equal to\n",
    "zero.  Our unknown is ${\\bf x}$, so we'll minimize in terms of that:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\frac{\\partial\\epsilon}{\\partial {\\bf x}} = 2{\\bf A}^T{\\bf Ax} - 2{\\bf A}^T{\\bf y} = 0,\\hspace{3cm} (11)\n",
    "\\end{equation}$$\n",
    "\n",
    "and this implies that\n",
    "\n",
    "$$\\begin{equation}\n",
    " {\\bf A}^T{\\bf Ax} = {\\bf A}^T{\\bf y}, \\hspace{3cm} (12)\n",
    "\\end{equation}$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\\begin{equation}\n",
    " {\\bf x} = ({\\bf A}^T{\\bf A})^{-1}{\\bf A}^T{\\bf y}.\\hspace{3cm} (13)\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exemple below we're gonna work with some fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Generate fake monthly SST data\n",
    "time = np.arange(1, 12*10+1) # each value is one month\n",
    "mean_temp = 16 \n",
    "annual_cycle = 5*(np.sin(2 * np.pi * (time / 12)) + np.cos(2 * np.pi * (time / 12))) \n",
    "trend = 0.05*time\n",
    "noise = 5*(np.random.rand(len(time)) - 0.5)\n",
    "temperature = mean_temp + trend + annual_cycle + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(time, temperature)\n",
    "plt.ylabel('SST C$^\\\\circ$', fontsize=14)\n",
    "plt.xlabel('months', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import inv\n",
    "nt = len(time)\n",
    "A = np.array([np.ones(nt), time]).T\n",
    "temperature = temperature.reshape([nt, 1])\n",
    "x = np.dot(inv(np.dot(A.T, A)), np.dot(A.T, temperature))\n",
    "fit = np.dot(A, x)\n",
    "print('Mean from fit:', x[0][0], 'Linear trend from fit:', x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(time, temperature, label = 'data')\n",
    "plt.plot(time, fit, 'r', label = 'Trend')\n",
    "plt.ylabel('SST C$^\\\\circ$', fontsize=14)\n",
    "plt.xlabel('months', fontsize=14)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've noted, if we want to find a trend, we define ${\\bf A}$ to have\n",
    "a column of ones (to identify the mean) and a column containing the time,\n",
    "to identify the rate of change.  We can make our model ${\\bf A}$\n",
    "progressively more complicated by adding additional columns.\n",
    "What do we do if we want to find the\n",
    "annual cycle?   Before I give you any answers, take a moment to think about\n",
    "this.\n",
    "\n",
    "We could use:\n",
    "$$\\begin{equation}\n",
    "   {\\bf A}= \\left[\\begin{array}{ccc}\n",
    "                              1 & \\cos(t_r) & \\sin(t_r) \\\\\n",
    "                              \\vdots & \\vdots & \\vdots  \\end{array}\\right],\\hspace{3cm} (14)\n",
    "\\end{equation}$$\n",
    "\n",
    "where time $t$ is measured in days, and $t_r=2\\pi t/365.25$, is the time in radians.\n",
    "We need the sine and cosine because we don't know the phase of\n",
    "our annual cycle exactly.  You might imagine that we could fit for the\n",
    "phase (e.g. $\\sin(t_r + \\phi)$), and we could, but that would be a non-linear\n",
    "fitting process, and the power of least-squares fitting won't work if we\n",
    "try that---we'd quickly be plunged into the murky world of non-linear\n",
    "fitting procedures, which is messy, unreliable, and not necessary in this case. Here's an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = np.array([np.ones(nt), time, np.sin(2*np.pi*time/12), np.cos(2*np.pi*time/12) ]).T \n",
    "x = np.dot(inv(np.dot(A2.T, A2)), np.dot(A2.T, temperature))\n",
    "fit = np.dot(A2, x)\n",
    "print('Mean from fit:', x[0][0], '\\nLinear trend from fit:', x[1][0],\n",
    "      '\\nAmplitude of sine:', x[2][0],'\\nAmplitude of cosine:', x[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(time, temperature, label = 'data')\n",
    "plt.plot(time, fit, 'r', lw=1, label = 'Fit')\n",
    "plt.ylabel('SST C$^\\\\circ$', fontsize=14)\n",
    "plt.xlabel('months', fontsize=14)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orthogonality and Least-Squares Fits\n",
    "\n",
    "Let's think about one important detail of our fitting procedure.\n",
    "What would happen if we wanted to fit $T=x_1 + x_2 t_r + x_3 t_r$, that\n",
    "is to fit two constants to the same variable?  In this case, clearly $x_2$\n",
    "and $x_3$ are completely indistinguishable.\n",
    "What happens when we try to use our redundant functions in our matrix ${\\bf A}$?\n",
    "$$\\begin{equation}\n",
    "{\\bf A}=\\left[\\begin{array}{ccc} 1 & t_1 & t_1 \\\\\n",
    "                           1 & t_2 & t_2 \\\\\n",
    "                           \\vdots & \\vdots & \\vdots \\\\\n",
    "                           1 & t_N & t_N \\end{array}\\right].\\hspace{3cm} (15)\n",
    "\\end{equation}$$\n",
    "\n",
    "Then we find:\n",
    "\n",
    "$$\\begin{equation}\n",
    "{\\bf A}^T{\\bf A} = \\left[\\begin{array}{ccc} N & \\sum t_i & \\sum t_i \\\\\n",
    "                               \\sum t_i & \\sum t_i^2 & \\sum t_i^2 \\\\\n",
    "                               \\sum t_i & \\sum t_i^2 & \\sum t_i^2\n",
    "        \\end{array}\\right].\\hspace{3cm} (16)\n",
    "\\end{equation}$$\n",
    "\n",
    "The second and third rows of ${\\bf A}^T{\\bf A}$ are identical, which tells us that\n",
    "the third row is adding no additional information to the system.  As a\n",
    "result, the matrix ${\\bf A}^T{\\bf A}$ is singular, and we won't be able to find an\n",
    "inverse for it.\n",
    "\n",
    "You probably wouldn't try to\n",
    "fit coefficients to two identical functions, but you might do something that\n",
    "was fairly similar.\n",
    "For example, $T=x_2 t_r + x_3 \\sin(t_r)$ poses a similar problem when $t_r$ is\n",
    "near zero.  In this case, the rows of ${\\bf A}^T{\\bf A}$ might not be identical, but\n",
    "they might be nearly the same so that Matlab would give you an error\n",
    "message.\n",
    "\n",
    "Similarly, you'll have trouble if you try:  $T=x_1 +x_2 t_r + x_3 (1+t_r)$.\n",
    "\n",
    "None of this was an issue when we used  sine and cosine, because\n",
    "they are orthogonal, so they contain no redundant information.\n",
    "\n",
    "##### Building complexity:  multiple oscillatory signals\n",
    "\n",
    "Now let's think about the pressure record from the Scripps pier, because\n",
    "we know that had a lot of sinusoidal variability.  What might we include\n",
    "in that fit?  We might hypothesize that data collected on the pier\n",
    "could be influenced by an annual cycle (1 cycle/365.25 days), a diurnal\n",
    "cycle (1 cycle/24 hours), and a tidal cycle (1 cycle/0.5175 days =\n",
    "1 cycle/(12 hours + 25.2 minutes)).  How would we fit for all of these\n",
    "components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions about pier data\n",
    "\n",
    "Here are some questions (yours plus some stray questions):\n",
    "\n",
    "1. How do temperature and salinity vary seasonally, annually, interannually, and on decadal time scales?  What accuracies do we need to assess these modes of variability?\n",
    "2. How have changes in the location of the pier influenced the long-term record?  What is the stability of the instrumentation?  Do error bars evolve over time?\n",
    "3. Does the pier create noise/flow distortion?\n",
    "4. How well do the manual and automated records match?\n",
    "5. Who quality controls the data, and how?\n",
    "6. Is there visibility data?\n",
    "7. Which of the variables associated with the pier record are measured directly, and which are inferred?\n",
    "8. What methods have been used to collect measurements on the pier, and how consistent are they? \n",
    "9. What variables are collected?  What is the formal uncertainty? What is the sampling frequency?\n",
    "10. When did automated sampling start?  When did automated sampling start being reliable?\n",
    "11. What are purposes of automated vs manual systems?}\n",
    "12. What level of adjustment is applied to make manual and automated data match?\n",
    "13. What has been published about these data?\n",
    "14. How often are the sensors serviced?\n",
    "15. What accounts for gaps in the data?\n",
    "16. How hands on is the automated system?  What's really automatized?\n",
    "17. What depths are the sensors?\n",
    "18. Where are the sensors?\n",
    "19. Are multiple sensors used are merged?\n",
    "20. What is the local geographic variability?\n",
    "21. What is the time of day of measurement?  How clearly is that documented?\n",
    "22. What does instrument failure look like in the data records?\n",
    "23. What error flags are available for the data?\n",
    "24. How long are records?\n",
    "25. How is equipment calibrated?  And how often?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
